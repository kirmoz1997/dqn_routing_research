# Research Plan — Multi-Agent Set Routing with DQN (v1.0.0)

## 1. Контекст и цель
Мы исследуем задачу **маршрутизации пользовательского запроса через сеть специализированных агентов**, где для каждого запроса требуется **не один агент**, а **набор (set) из 2–9 агентов**.  
Цель — научиться выбирать **оптимальный набор агентов** для решения запроса, избегая:
- **недобора** (не выбрали нужных агентов),
- **перебора** (выбрали лишних агентов).

Порядок вызова агентов **не важен**. Важен итоговый набор.

## 2. Агенты (9 классов)
Каждый агент — узкая специализация:

0. Code Agent (Python)  
1. SQL Agent  
2. Data Analysis Agent (Pandas)  
3. Math Formula Solver  
4. Structured Extraction Agent (JSON)  
5. Summarization & Formatting Agent  
6. Requirements / ТЗ Agent  
7. Rewrite / Style Constraints Agent  
8. Finance / Numeric Computation Agent  

## 3. Постановка задачи (multi-label routing)
Для каждого запроса задан истинный набор требуемых агентов `R` (|R| ∈ [2, 9]).  
Роутер выбирает набор агентов `S` (|S| ∈ [2, 9]) через последовательный выбор действий.

Ошибки:
- **Coverage (покрытие)**: `|S ∩ R|`
- **Over-selection (перебор)**: `|S \ R|`
- **Under-selection (недобор)**: `|R \ S|`

## 4. Формализация как MDP (для DQN)
Задача формализуется как MDP (процесс принятия решений), где агент-роутер последовательно выбирает агентов.

### 4.1 Состояние (State)
Состояние включает:
- текст запроса `text`,
- маску выбранных агентов `selected_mask` (длина 9, 0/1).

### 4.2 Действия (Actions)
- выбрать одного из 9 агентов, которого ещё не выбирали,
- специальное действие `STOP` (завершить выбор набора).

Повторный выбор агента запрещён (или штрафуется как ошибка; в v1.0.0 считаем запрещённым).

### 4.3 Ограничение по шагам
- `max_steps = 9` (не больше числа уникальных агентов).
- `STOP` может быть выполнен на любом шаге.

## 5. Модель качества агента (симуляция)
Мы не вызываем реальный LLM в основной симуляции качества.  
Предполагается, что:
- если выбранный агент принадлежит `R`, то он "срабатывает" с вероятностью `p_good = 0.85`,
- если агент не принадлежит `R`, штраф применяется с вероятностью `p_bad = 0.30`.

В v1.0.0 шаг "полезен", если выбран агент из `R` и он ещё не был покрыт ранее.

## 6. Reward (награда) — оптимизация набора
Награда определяется так, чтобы:
- поощрять выбор нужных агентов,
- штрафовать лишних,
- штрафовать недобор после завершения.

Параметры (стартовые):
- `alpha = +1.0` за покрытие нового нужного агента,
- `beta = 0.5` штраф за выбор лишнего агента,
- `gamma = 1.0` штраф за каждый пропущенный нужный агент при завершении.
- `step_cost = 0.0` штраф за каждый шаг выбора агента (стимулирует более ранний `STOP`).

### 6.1 Reward по шагам
На каждом шаге при выборе агента `a` (и если это не `STOP`):
- `reward -= step_cost` (штраф за шаг, поощряет быстрый выбор)
- если `a ∈ R` и ещё не покрыт: `reward += alpha` (с вероятностью `p_good`)
- если `a ∉ R`: `reward -= beta` (с вероятностью `p_bad`)
- если `a ∈ R`, но уже был выбран: `reward += 0`

### 6.2 Reward на STOP / завершение эпизода
После `STOP` или достижения `max_steps`:
- `missing = |R \ S|`
- `reward -= gamma * missing`

Итоговая цель — максимизировать суммарный reward за эпизод.

## 7. Датасет

### 7.1 Размер
Целевой размер для старта: ~300 запросов.  
Текущий размер: **323 запроса** (`data/tasks_set.jsonl`).

### 7.2 Баланс
Примеры балансируются по:
- размеру `|R|` (запросы, требующие 2..9 агентов),
- типам сигналов в тексте.

### 7.3 Формат данных
Исходный формат редактирования: `TSV` (удобно править руками).  
Финальный формат для кода: `JSONL`.

**TSV (draft):** `data/tasks_set_draft.tsv`  
Поля:
- `id`
- `required_agents` (строка вида `0,2,4` — уникальные id, 2..9 шт)
- `difficulty` (исторический столбец; **удаляется** при конвертации в JSONL)
- `eval_hint`
- `text`
- `notes`

**JSONL (canonical):** `data/tasks_set.jsonl`  
Одна строка = один JSON-объект. Поле `difficulty` **не включается** в JSONL.

Пример записи:
```json
{"id":"ex_0001","required_agents":[0,2,4],"eval_hint":"код + JSON извлечение","text":"...","notes":"..."}
```

Каноничный формат `required_agents`:
- список `int`, отсортированный по возрастанию, без повторов,
- каждый элемент в диапазоне 0..8,
- длина от 2 до 9.

### 7.4 Разбиение на сплиты
Скрипт `tools/split_jsonl_set.py` разбивает датасет на train / val / test.

- Поддерживается **стратифицированный** split по `k = len(required_agents)` (по умолчанию).
- Гарантируется, что каждый bucket `k` представлен в val и test (при наличии >= 3 записей в группе).
- Доли по умолчанию: train=0.70, val=0.15, test=0.15.
- Результат: `data/splits/{train,val,test}.jsonl`.

## 8. Baseline-модели (4 шт)
Во всех baseline результат — это набор агентов `S` (полученный последовательным выбором без повторов + STOP/лимит).

1) **Random Routing** (`run_random_set.py`)  
Случайно выбирает агентов до остановки/лимита (без повторов).

2) **Rule-Based Routing** (`run_rule_set.py`)  
Извлекает агентов по триггерным словам/паттернам.  
Если выбрано слишком мало — добивает случайными.  
Если слишком много — обрезает по приоритетам.

3) **Supervised Router** (`run_supervised_set.py`)  
TF-IDF (1,2)-gram + OneVsRest(LogisticRegression) → multi-hot → agent set.  
Порог отсечения выбирается sweep-ом на val по максимуму mean_f1.  
Гарантируется минимум 2 агента, максимум 9.

4) **LLM-Router (API + prompt)** (`run_llm_set.py`)  
Реальный LLM по системному промпту возвращает список agent_id (2..9) в JSON-формате.  
Промпт — recall-biased: приоритет на полноту (не пропустить нужного агента), с внутренним чеклистом подзадач.  
Версия промпта фиксируется константой `PROMPT_VERSION` (текущая: `v2-recall-biased`).

**Кэширование:**  
- Кэш хранится в JSONL-файле (по умолчанию `cache/llm_router_cache.jsonl`).  
- Ключ кэша: `id` элемента, фильтрация по `model` + `prompt_version`.  
- При смене модели или версии промпта старый кэш автоматически игнорируется — элементы перезапрашиваются через API.  
- Каждая запись в кэше содержит: `id`, `pred`, `raw`, `model`, `prompt_version`.

**Обработка ошибок:**  
- До `max_retries` повторных запросов при сбое API или невалидном JSON.  
- Keyword fallback при полном провале парсинга (гарантирует ≥ 2 агента).

## 9. Основной метод: DQN
DQN обучается выбирать действия (агент или STOP), максимизируя ожидаемый суммарный reward.  
Состояние включает текст и маску уже выбранных агентов.

## 10. Метрики оценки

### 10.1 Основные метрики (global)
Для каждого метода считаем (на тестовом наборе):

| Метрика | Описание |
|---|---|
| `mean_episode_reward` | Средняя суммарная награда за эпизод |
| `success_rate` | Доля задач, где `missing = 0` (все нужные агенты покрыты) |
| `exact_match_rate` | Доля задач, где `set(S) == set(R)` (точное совпадение наборов) |
| `mean_jaccard` | Среднее Jaccard: `|S ∩ R| / |S ∪ R|` |
| `mean_precision` | Среднее Precision: `|S ∩ R| / |S|` |
| `mean_recall` | Среднее Recall: `|S ∩ R| / |R|` |
| `mean_f1` | Среднее F1: `2 * precision * recall / (precision + recall)` |
| `avg_steps` | Средняя длина выбранного набора |
| `avg_coverage` | Среднее `|S ∩ R|` |
| `avg_overselection` | Среднее `|S \ R|` |
| `avg_underselection` | Среднее `|R \ S|` |

### 10.2 Метрики по bucket-ам
Все метрики из §10.1 рассчитываются дополнительно по трём группам сложности (по размеру целевого набора):

| Bucket | Размеры `|R|` | Характеристика |
|---|---|---|
| **A** | {2, 3} | Малые наборы — проще выбрать, выше риск over-selection |
| **B** | {4, 5, 6} | Средние наборы — основная масса данных |
| **C** | {7, 8, 9} | Большие наборы — выше риск under-selection (ранний STOP) |

### 10.3 Выбор порога (Supervised Router)
Sweep threshold на val-сплите с ранжированием:
1. max `mean_f1`
2. max `mean_jaccard`
3. max `mean_episode_reward`
4. min `avg_steps`

## 11. Воспроизводимость
- Фиксируем сиды для генераторов случайности.
- Сохраняем версию датасета и параметры reward в репозитории.
- Все эксперименты запускаются скриптами из `src/multiagent_dqn_routing/experiments/`.
- Стратифицированный split гарантирует представленность всех размеров `|R|` в каждом сплите.
- LLM-Router: версия промпта (`PROMPT_VERSION`) фиксируется в коде и записывается в кэш. При смене версии кэш автоматически инвалидируется (старые записи игнорируются по несовпадению `prompt_version`).